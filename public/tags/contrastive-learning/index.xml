<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Contrastive Learning | Ahmed Abdou</title>
    <link>http://localhost:1313/tags/contrastive-learning/</link>
      <atom:link href="http://localhost:1313/tags/contrastive-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Contrastive Learning</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 01 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png</url>
      <title>Contrastive Learning</title>
      <link>http://localhost:1313/tags/contrastive-learning/</link>
    </image>
    
    <item>
      <title>Triplet Loss Function for embedding space learning</title>
      <link>http://localhost:1313/post/triplet_loss_function/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/triplet_loss_function/</guid>
      <description>&lt;p&gt;In this blog, a full guide for the triplet loss function that gained special attention lately for its importance in face recognition and verification tasks. The blog discuss the triplets variations and different mining techniques. Then, some advanced notes about the soft margin from &lt;a href=&#34;https://arxiv.org/pdf/1703.07737.pdf&#34;&gt;1&lt;/a&gt;, and Improved triplet loss from &lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2016/papers/Cheng_Person_Re-Identification_by_CVPR_2016_paper.pdf&#34;&gt;2&lt;/a&gt;. Finally, the visualization of trained embeddings from the GitHub Gist &lt;a href=&#34;https://github.com/AhmedAbdel-Aal/DeepLearning_home/blob/main/Triplet%20Loss/Mnist_Embeddings_Visualization_using_Semi_Hard_Triplet_Loss.ipynb&#34;&gt;Mnist Embeddings Visualization using Semi-Hard Triplet Loss&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;table-of-contents&#34;&gt;table of contents&lt;/h2&gt;
&lt;p&gt;1- &lt;a href=&#34;#1-what-is-triplet-loss-function&#34;&gt;what is triplet loss function&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;​			- &lt;a href=&#34;#siamese-network&#34;&gt;Siamese Network&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;​			- &lt;a href=&#34;triplet-loss-definition&#34;&gt;Triplet Loss Definition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2- &lt;a href=&#34;#2-triplets-variation&#34;&gt;Triplets variation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3- &lt;a href=&#34;#3-triplets-mining-techniques&#34;&gt;Triplets mining techniques&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;4- &lt;a href=&#34;#4-advanced-notes&#34;&gt;Advanced Notes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;​     	- &lt;a href=&#34;#soft-margin&#34;&gt;Soft margin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;​		 - &lt;a href=&#34;#improved-triplet-loss&#34;&gt;Improved triplet loss&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;​		 - &lt;a href=&#34;#embedding-visualization&#34;&gt;Embedding visualization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;5-&lt;a href=&#34;#5-resources&#34;&gt;Resources&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-what-is-triplet-loss-function&#34;&gt;1-What is Triplet Loss Function&lt;/h2&gt;
&lt;p&gt;Triplet loss is always accompanied with face recognition, verification tasks. Triplet loss allows the option of dealing with variable number of classes, as in face recognition. Face recognition is the question of &amp;ldquo;who is this person from all the persons in the data-set we have&amp;rdquo;. Triplet loss also raised with Siamese networks architecture.&lt;/p&gt;
&lt;h3 id=&#34;siamese-networks&#34;&gt;Siamese Networks&lt;/h3&gt;
&lt;p&gt;Siamese network is the type of network that have more than one instance of the same model, to learn similarities and differences between data inputs. To illustrate more, assume we want to build face recognition model, then Siamese network is a good choice, and it would work in the following manner:&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s create a model that takes an image as inputs and output an embedding of that image.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;s.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then let&amp;rsquo;s create two instances of that model, and pass the two corresponding embeddings into a similarity function. If the two images are of the &lt;em&gt;&lt;strong&gt;same person&lt;/strong&gt;&lt;/em&gt;, then they should have &lt;em&gt;&lt;strong&gt;high similarity&lt;/strong&gt;&lt;/em&gt;, and &lt;em&gt;&lt;strong&gt;low similarity&lt;/strong&gt;&lt;/em&gt; if the two images are of &lt;em&gt;&lt;strong&gt;different persons&lt;/strong&gt;&lt;/em&gt;. Now, we can define that the loss is the complement of the similarity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./siamese.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;triplet-loss-definition&#34;&gt;Triplet Loss Definition&lt;/h3&gt;
&lt;p&gt;The aim of the triplet loss is to make sure of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The instances from the same class are pulled near each other&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The instances from different classes are pushed away from each other.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To explain how the previous two points are achieved by the triplet loss, let&amp;rsquo;s assume we have a neural network that works as an embedding function. Assume that, an instance from our data-set is x, and the embedding function is f(x) that result in an embeddings vector X. By the same concept, another instance y would result in Y vector after being passed through f(y). d(X,Y) is a function that calculates the distance between the two vectors X and Y. If x and y belong to the same class, then our aim is to make this distance small as possible, and the opposite otherwise.&lt;/p&gt;
&lt;p&gt;To formalize the previous requirements, we need to pick three embeddings with the following specific properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;anchor&lt;/strong&gt;&lt;/em&gt;, A = f(a).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;positive&lt;/strong&gt;&lt;/em&gt;, P = f(p). Another instance from the same class as the anchor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;negative&lt;/strong&gt;&lt;/em&gt;, N = f(n). Another instance from a different class than the anchor.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That&amp;rsquo;s from where the name &amp;ldquo;triplets&amp;rdquo; came. Then, we need to minimize the distance d(A,P), and maximize the distance d(A,N). To do so:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./derivation%20without%20margin.PNG&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;The previous inequality is satisfied when the d(A,N) is bigger than the d(A,P). However, there is a trivial solution that also satisfies the equation, which is both the distances are equal to zero. To prevent the trained model from this trivial solution, a new parameter called &amp;ldquo;margin&amp;rdquo; is introduced, so that the new derivation would be like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;derivation%20with%20margin.PNG&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;To formulate this requirement into a function, the hinge function is used. The hinge function assure that if the triplets already follow our requirements, then no loss (zero loss) is detected.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;TL.PNG&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;A practical intuition of what triplet loss do in face recognition is, if we applied the triplet loss for training the Siamese networks mentioned above, then the embeddings for two &lt;em&gt;&lt;strong&gt;different images&lt;/strong&gt;&lt;/em&gt; for the &lt;em&gt;&lt;strong&gt;same person&lt;/strong&gt;&lt;/em&gt; have a small distance between each other, and both of them having a bigger distances with the embeddings for the images of another &lt;em&gt;&lt;strong&gt;different person&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.hashnode.com/res/hashnode/image/upload/v1643160063974/QTfxpsSUB.png?auto=compress,format&amp;amp;format=webp&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;2-triplets-variation&#34;&gt;2-Triplets variation&lt;/h1&gt;
&lt;p&gt;Triplets variations means that there are more than one type of triplets that you can collect from your data. First of all, we have a base rule that defines that triplets as anchor, positive example and negative example. The variations comes when which exactly positive examples from all the positives that you can choose at each time, and which exactly the negative example that you can choose from all the negative examples that you have.&lt;/p&gt;
&lt;p&gt;according to our rule:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./TL.PNG&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;we have three kind of triplets that we can generate :&lt;/p&gt;
&lt;p&gt;- &lt;em&gt;&lt;strong&gt;Easy Triplets&lt;/strong&gt;&lt;/em&gt; these triplets results in zero loss because they are already satisfying the loss function i.e., the negative example has bigger distance than the positive example plus the margin&lt;/p&gt;
&lt;p&gt;​&lt;img src=&#34;easy-triplets.PNG&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;- &lt;em&gt;&lt;strong&gt;Semi-Hard Triplets&lt;/strong&gt;&lt;/em&gt; the negative example is not closer to the anchor more than the positive, but the negative example has smaller distance than the positive example plus the margin&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;semi-hard%20triplets.PNG&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;- &lt;em&gt;&lt;strong&gt;Hard Triplets&lt;/strong&gt;&lt;/em&gt; The nearest negative example and the farthest positive example to the anchor are chosen&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;hard%20triplets.PNG&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;3-triplets-mining-techniques&#34;&gt;3-Triplets mining techniques&lt;/h1&gt;
&lt;p&gt;Whatever the kind of triplets you have chosen to generate, you need to choose between two different generation approaches, offline and online mining. The core difference between them is, with offline mining you generate the triplets before the training process from your original data. While in online mining, you generate the triplets after passing your data through your embedding network and works on the embeddings themselves rather than the data examples. To get in more details for each approach:&lt;/p&gt;
&lt;h2 id=&#34;offline-mining&#34;&gt;Offline mining&lt;/h2&gt;
&lt;p&gt;You can assume that Offline mining approach is like a Siamese network with three instances of the same embedding network. Each one of the three networks takes the anchor, positive and negative examples respectively. After computing the embeddings from each example, we calculate the required distances, then the triplet loss.&lt;/p&gt;
&lt;p&gt;First we create list of triplets (a,p,n), whatever the type of triplets we need to generate, then we divide them into N batches. For each batch, we pass triplets one after the other, resulting in their corresponding embeddings. So we make N passes for one epoch.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;offline-mining.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;One point can be added here, is that we can update the distribution of the triplets over the N batches after each epoch.&lt;/p&gt;
&lt;p&gt;The drawbacks of this approach are :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;you need to make a full pass on the data-set before the training.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;you may need to update the mined batches after each epochs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is not clear which of the triplets you generated are semi-hard and which are hard, because the type of triplets depends on the distance function, which in turns depends on the embedding vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The memory and computation constraints due to training 3 identical networks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;online-mining&#34;&gt;Online mining&lt;/h2&gt;
&lt;p&gt;The idea of Online mining is to overcome the computation and memory drawback of the offline mining. Thus the triplets mining is applied over the embeddings of the data, not the data itself. Therefore the architecture of the model is changed into:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;offline-mining.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;First, we pass a 3N batch of input data through the embedding network to calculate 3N embedding vectors. Secondly, we create a pair-wise distance matrix for all the embedding vectors. Third, for each vector we chose a positive and negative examples to calculate the loss. Finally, we accumulate the loss from all the triplets to be out batch loss.&lt;/p&gt;
&lt;p&gt;Worth to mention that the number of valid triplets we can get from a batch depends on how the data are distributed over the batches. In best case scenario, If we passed 3N input Batch to the embedding network, we can get at most N triplets. Thus we can divide the data classes fairly over the batches before training, but this would not be the best solution in case of imbalanced data.&lt;/p&gt;
&lt;p&gt;There are two types of online triplets mining strategies introduced in &lt;a href=&#34;https://arxiv.org/pdf/1703.07737.pdf&#34;&gt;In Defense of the Triplet Loss for Person Re-Identification&lt;/a&gt;. They are named &lt;code&gt;Batch All&lt;/code&gt; and &lt;code&gt;Batch Hard&lt;/code&gt;. An implementation for the two strategies are implemented in the great blog post by Olivier Moindrot &lt;a href=&#34;https://omoindrot.github.io/&#34;&gt;Triplet Loss and Online Triplet Mining in TensorFlow&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;batch-hard&#34;&gt;Batch Hard&lt;/h3&gt;
&lt;p&gt;Select the hardest positive (farthest positive) and the hardest negative (nearest negative) for each anchor within its batch. The also have been referred by &lt;strong&gt;moderate triplets&lt;/strong&gt;, as they are the hardest but within its batch only, not over all the data-set.&lt;/p&gt;
&lt;h3 id=&#34;batch-all&#34;&gt;Batch All&lt;/h3&gt;
&lt;p&gt;Select all the possible combination of triplets (hard, and semi-hard).&lt;/p&gt;
&lt;h1 id=&#34;4-advanced-notes&#34;&gt;4-Advanced Notes&lt;/h1&gt;
&lt;h4 id=&#34;soft-margin&#34;&gt;Soft margin&lt;/h4&gt;
&lt;p&gt;Imagine a triplets where the condition  &lt;code&gt;d(A,N) &amp;gt;= d(A,P) + margin&lt;/code&gt;  is already satisfied, then according to triplet loss rule that will result in zero. The zero we got is due to the hinge function  &lt;strong&gt;F(x) = max(x,0)&lt;/strong&gt;. But, what if we want to further make the gap wider. In other words, push the negative example more farther and pull the positive example more closer.&lt;/p&gt;
&lt;p&gt;From &lt;a href=&#34;https://arxiv.org/pdf/1703.07737.pdf&#34;&gt;In Defense of the Triplet Loss for Person Re-Identification&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The role of the hinge function [m + •]+ is to avoid correcting “already correct” triplets. But in person ReID, it can be beneficial to pull together samples from the same class as much as possible&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ReID: person re-identification.&lt;/p&gt;
&lt;p&gt;To do so, the hinge function is replaced by another function, called soft plus &lt;strong&gt;F(x) = 1+e&lt;sup&gt;x&lt;/sup&gt;&lt;/strong&gt; . This function knows as &lt;code&gt;log1p&lt;/code&gt; in numpy library. check &lt;a href=&#34;https://numpy.org/doc/stable/reference/generated/numpy.log1p.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The reason for the soft margin naming came from &lt;a href=&#34;https://arxiv.org/pdf/1703.07737.pdf&#34;&gt;In Defense of the Triplet Loss for Person Re-Identification&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.hashnode.com/res/hashnode/image/upload/v1643160392810/dOkCbm_7y.jpeg?auto=compress,format&amp;amp;format=webp&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The softplus function has similar behavior to the hinge, but it decays exponentially instead of having a hard cut-off, we hence refer to it as the soft-margin formulation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;improved-triplet-loss&#34;&gt;Improved triplet loss:&lt;/h4&gt;
&lt;p&gt;In &lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2016/papers/Cheng_Person_Re-Identification_by_CVPR_2016_paper.pdf&#34;&gt;Person Re-Identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function&lt;/a&gt; a new approach of calculating the triplet loss is proposed. The reason for new proposed way is to make sure that each class cover a reasonable intra-class space in the new learned space.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;instances belonging to the same person may form a large cluster with a relatively large average intra-class distance in the learned feature space. Clearly, this is not a desired outcome, and will inevitably hurt the person re-id performance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For this reason a new margin &lt;strong&gt;m2&lt;/strong&gt; which is much smaller than the first margin &lt;strong&gt;m1&lt;/strong&gt; is used to pull the instances  of the same class more closer. The new triplet loss equation would be:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;improved%20TL.PNG&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The previous equation is from &lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2016/papers/Cheng_Person_Re-Identification_by_CVPR_2016_paper.pdf&#34;&gt;here&lt;/a&gt;, and it is modified to match the blog notations.&lt;/p&gt;
&lt;h4 id=&#34;embedding-visualization&#34;&gt;Embedding visualization&lt;/h4&gt;
&lt;p&gt;One way to gain intuition about the pushing and pulling between the embeddings vectors is to visualize them over the training process. Usually the embedding vector is of a high dimension, for example in &lt;a href=&#34;&#34;&gt;FaceNet paper&lt;/a&gt; the embedding vector length was 128.  Hence, a dimensionality reduction technique is applied to reduce the embedding vector size to 2 or 3 dimensions. principal component analysis (&lt;strong&gt;PCA&lt;/strong&gt;) and  t-distributed stochastic neighbor embedding (&lt;strong&gt;t-SNE&lt;/strong&gt;) are the most popular ways used in high dimension vectors visualization.&lt;/p&gt;
&lt;p&gt;In this GitHub Gist &lt;a href=&#34;https://gist.github.com/AhmedAbdel-Aal/794d6f9c004bd07762975db580734a44&#34;&gt;Mnist Embeddings Visualization using Semi-Hard Triplet Loss&lt;/a&gt;, the Mnist data-set are passed through an Embedding network that is trained via &lt;a href=&#34;https://www.tensorflow.org/addons/api_docs/python/tfa/losses/TripletSemiHardLoss&#34;&gt;tensorflow addons Semi-Hard Triplet loss&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here is the architecture from the embedding network used, taken from &lt;a href=&#34;https://www.tensorflow.org/addons/api_docs/python/tfa/losses/TripletSemiHardLoss&#34;&gt;tensorflow addons Semi-Hard Triplet loss&lt;/a&gt;. One thing to highlight is the last layer in the network, which is &lt;strong&gt;L2_norm&lt;/strong&gt; layer. This layer is added so that the Euclidian norm of the embedding vector is 1. This in turn forces the embeddings to be on a hypersphere.&lt;/p&gt;
&lt;p&gt;From &lt;a href=&#34;https://arxiv.org/pdf/1503.03832.pdf&#34;&gt;FaceNet&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Additionally, we constrain this embedding to live on the d-dimensional hypersphere, i.e. |f(x)|&lt;sub&gt;2&lt;/sub&gt; = 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;embedding_network = tf.keras.Sequential([
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;, input_shape=input_shape),
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        tf.keras.layers.MaxPooling2D(pool_size=2),
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        tf.keras.layers.Dropout(0.3),
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;),
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        tf.keras.layers.MaxPooling2D(pool_size=2),
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        tf.keras.layers.Dropout(0.3),
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        tf.keras.layers.Flatten(),
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        tf.keras.layers.Dense(embedding_size, activation=None), # No activation on final dense layer
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    ])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Two principal components of these embedding vectors are then visualized before and after training.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;before-after.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;Also the embeddings are plotted before each epoch to monitor how the triplet-loss affects the embeddings of the same class to be pulled towards each other, and the dissimilar classes to be pushed away from each other.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Embeddings_over_training.gif&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;5-resources&#34;&gt;5-Resources&lt;/h1&gt;
&lt;p&gt;1-  &lt;a href=&#34;https://arxiv.org/pdf/1703.07737.pdf&#34;&gt;In Defense of the Triplet Loss for Person Re-Identification&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2- &lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2016/papers/Cheng_Person_Re-Identification_by_CVPR_2016_paper.pdf&#34;&gt;Person Re-Identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3- Blog post by Olivier Moindrot &lt;a href=&#34;https://omoindrot.github.io/&#34;&gt;Triplet Loss and Online Triplet Mining in TensorFlow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;4- Blog post Brandon Amos &lt;a href=&#34;http://bamos.github.io/2016/01/19/openface-0.2.0/&#34;&gt;OpenFace 0.2.0: Higher accuracy and halved execution time&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;5- &lt;a href=&#34;https://arxiv.org/pdf/1503.03832.pdf&#34;&gt;FaceNet: A Unified Embedding for Face Recognition and Clustering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;6- &lt;a href=&#34;https://www.tensorflow.org/addons/api_docs/python/tfa/losses/TripletSemiHardLoss&#34;&gt;tensorflow addons Semi-Hard Triplet loss&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
